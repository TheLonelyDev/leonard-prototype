{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-889178bea78c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaledArousal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;31m# Plot the initial dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-889178bea78c>\u001b[0m in \u001b[0;36mStandardScaledArousal\u001b[1;34m()\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mStandardScaledArousal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLoadDatafile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mStandardScaling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-889178bea78c>\u001b[0m in \u001b[0;36mLoadDatafile\u001b[1;34m()\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mLoadDatafile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;31m# Load the json file into a pandas dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;31m# Drop the filename col\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 717\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    718\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m    737\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"frame\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 739\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    740\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"series\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 849\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    851\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1091\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"columns\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m             self.obj = DataFrame(\n\u001b[1;32m-> 1093\u001b[1;33m                 \u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1094\u001b[0m             )\n\u001b[0;32m   1095\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"split\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected object or value"
     ],
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error"
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy\n",
    "\n",
    "def load(file):\n",
    "    y, sr = librosa.load(file, mono=True, duration=10)\n",
    "    \n",
    "    onset_env = librosa.onset.onset_strength(y, sr=sr)\n",
    "    out = {\n",
    "        'filename': file,\n",
    "        'arousal': 0,\n",
    "        'valence': 0,\n",
    "        # Mel-based Power Spectrogram\n",
    "        'melspectrogram': numpy.mean(librosa.feature.melspectrogram(y=y, sr=sr)),\n",
    "        # Tempo features\n",
    "        'tempo': numpy.mean(librosa.beat.tempo(onset_envelope=onset_env, sr=sr)),\n",
    "        'tempo_std': numpy.std(librosa.beat.tempo(onset_envelope=onset_env, sr=sr, aggregate=None)),\n",
    "\n",
    "        # Zero Cross Rating\n",
    "        'zcr': numpy.mean(librosa.feature.zero_crossing_rate(y)),\n",
    "        'zcr_std': numpy.std(librosa.feature.zero_crossing_rate(y)),\n",
    "\n",
    "        # Octave contrast std\n",
    "        'spectral_contrast_std': numpy.std(librosa.feature.spectral_contrast(y=y, sr=sr))\n",
    "    }\n",
    "\n",
    "    # Extract all Mel-frequency cepstral coefficients\n",
    "    counter = 1\n",
    "    for mfcc in (librosa.feature.mfcc(y=y, sr=sr)):\n",
    "        out[('mfcc%s' % counter)] = numpy.mean(mfcc)\n",
    "        counter = counter + 1\n",
    "\n",
    "    # Chromagram of a Short Time Fourier Transform\n",
    "    counter = 1\n",
    "    for chroma_stft in (librosa.feature.chroma_stft(y=y, sr=sr)):\n",
    "        out[('chroma_stft%s' % counter)] = numpy.mean(chroma_stft)\n",
    "        counter = counter + 1\n",
    "\n",
    "    # Octave-based Spectral Contrast\n",
    "    counter = 1\n",
    "    for spectral_contrast in (librosa.feature.spectral_contrast(y=y, sr=sr)):\n",
    "        out[('spectral_contrast%s' % counter)] = numpy.mean(spectral_contrast)\n",
    "        counter = counter + 1\n",
    "\n",
    "    # Tonnetz\n",
    "    counter = 1\n",
    "    for tonnetz in (librosa.feature.tonnetz(y=y, sr=sr)):\n",
    "        out[('tonnetz%s' % counter)] = numpy.mean(tonnetz)\n",
    "        counter = counter + 1\n",
    "\n",
    "    return out\n",
    "\n",
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Datafile\n",
    "datafile = './data3.json'\n",
    "\n",
    "# Imports\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "import json\n",
    "import numpy\n",
    "import pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "#import required packages\n",
    "from sklearn import neighbors\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "def base(title):\n",
    "    plt.figure()\n",
    "    # Set x-axis range\n",
    "    plt.xlim((-5,5))\n",
    "    # Set y-axis range\n",
    "    plt.ylim((-5,5))\n",
    "    # Draw lines to split quadrants\n",
    "    plt.plot([0,0],[-5,5], linewidth=1, color='black')\n",
    "    plt.plot([5,-5],[0,0], linewidth=1, color='black')\n",
    "    plt.title(title)\n",
    "    \n",
    "    return plt\n",
    "    \n",
    "def scatter(plt, arousal, valence):\n",
    "    plt.scatter(arousal, valence)\n",
    "\n",
    "def plot(title, data):\n",
    "    p = base(title)\n",
    "    for sub in data:\n",
    "        scatter(p, sub[0], sub[1])\n",
    "    p.show()\n",
    "\n",
    "def LoadDatafile():\n",
    "    # Load the json file into a pandas dataframe\n",
    "    data = pandas.read_json(datafile)   \n",
    "    \n",
    "    # Drop the filename col\n",
    "    data = data.drop(['filename'], axis=1)\n",
    "    \n",
    "    #diff = data.valence.ptp()\n",
    "    #range = diff/4\n",
    "    #lower = range*2\n",
    "    #upper = range*3\n",
    "    #print(diff, range, lower, upper)\n",
    "    # 2 < 3 < 5 = TRUE\n",
    "    # 2 < 6 < 5 = FALSE\n",
    "    \n",
    "    #indexNames = data[ (lower < data.arousal) & (data.arousal < upper)].index\n",
    "    #print(indexNames)\n",
    "    # Delete these row indexes from dataFrame\n",
    "    #data.drop(indexNames , inplace=True)\n",
    "    \n",
    "    #with pandas.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    #    print(data.valence)    \n",
    "    \n",
    "    from sklearn.utils import shuffle\n",
    "    data = shuffle(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Create scalers\n",
    "scalerX = StandardScaler()\n",
    "scalerY = StandardScaler()\n",
    "def StandardScaling(x, y):\n",
    "    # Determine the data\n",
    "    #   x = all data without arousal & valence\n",
    "    #   y = arousal & valence\n",
    "    \n",
    "    # Scale the datasets & convert to numpy arrays\n",
    "    x = scalerX.fit_transform(numpy.array(x, dtype = float))\n",
    "    y = scalerY.fit_transform(numpy.array(y, dtype = float))\n",
    "    #x = numpy.array(x, dtype = float)\n",
    "    #y = numpy.array(y, dtype = float)\n",
    "    print(x)\n",
    "    # Split into train & test datasets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def StandardScaledAV():\n",
    "    data = LoadDatafile()\n",
    "    \n",
    "    return StandardScaling(data.iloc[:, 2:] , data.iloc[:, :2])\n",
    "\n",
    "def StandardScaledArousal():\n",
    "    data = LoadDatafile()\n",
    "    \n",
    "    return StandardScaling(data.iloc[:, 2:] , data.iloc[:, :1])\n",
    "\n",
    "def StandardScaledValence():\n",
    "    data = LoadDatafile()\n",
    "    \n",
    "    return StandardScaling(data.iloc[:, 2:] , data.iloc[:, 1:2])\n",
    "\n",
    "# Function to determine the best k value\n",
    "def getK(x, y):\n",
    "    params = {'n_neighbors':[]}\n",
    "    for i in range(2, 20):\n",
    "        params['n_neighbors'].append(i)\n",
    "    \n",
    "    knn = neighbors.KNeighborsRegressor()\n",
    "    model = GridSearchCV(knn, params, cv=5)\n",
    "    model.fit(x, y)\n",
    "    \n",
    "    return model.best_params_ \n",
    "\n",
    "def KNN(x):\n",
    "    model = neighbors.KNeighborsRegressor (n_neighbors = 10)#17 = optimalgetK(x_train, y_train))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def MeanSquared(x, opt):\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        layers.Dense(37, activation='relu', input_shape=(x.shape[1],), kernel_initializer='normal'),\n",
    "        layers.Dropout(.25),\n",
    "        layers.Dense(18, activation='relu'),\n",
    "        layers.Dense(9, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", loss='mean_squared_logarithmic_error', metrics=['mse','mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def RefitArousalY(y):\n",
    "    return numpy.insert(numpy.array(y), 0, 0, axis=1)\n",
    "\n",
    "def RefitValenceY(y):\n",
    "    return numpy.insert(numpy.array(y), 1, 0, axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = StandardScaledArousal()\n",
    "    \n",
    "# Plot the initial dataset\n",
    "plot(\"Y_train, scaled data; no training (raw)\", RefitArousalY(y_train))\n",
    "\n",
    "files = [\"fast car.mp3\", \"sultans of swing.mp3\", \"relaxed.mp3\", \"happy.mp3\", \"ph happy.mp3\", \"stranded.mp3\"]\n",
    "\n",
    "def compute(modelname, model, file):\n",
    "    loaded_data = load('./%s' % file)\n",
    "    dataframe = pandas.DataFrame.from_dict(loaded_data, orient='index').T\n",
    "    dataframe = dataframe.drop(['filename', 'arousal', 'valence'], axis=1)\n",
    "    \n",
    "    dataframe = scalerX.transform(numpy.array(dataframe, dtype=float))\n",
    "    \n",
    "    predictions = model.predict(dataframe)\n",
    "    #predictions = scalerY.inverse_transform(predictions)\n",
    "    \n",
    "    plot(\"[%s] %s (%s)\" % (modelname, file, predictions), RefitArousalY(predictions))\n",
    "\n",
    "models = {\n",
    "    #'KNN': [KNN(x_train), 0],\n",
    "    'MSQ-ADAM': [MeanSquared(x_train, 'adam'), 100]\n",
    "}    \n",
    "\n",
    "for modelname, _model in models.items():\n",
    "    # Create model & fit\n",
    "    history = 1\n",
    "    model = _model[0]\n",
    "    if _model[1] is 0:\n",
    "        model.fit(x_train, y_train) \n",
    "    else:\n",
    "        history = model.fit(x_train, y_train, epochs=200, verbose=False, validation_data=(x_test, y_test))\n",
    "        print(history.history.keys())\n",
    "        # \"Loss\"\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc='upper left')\n",
    "        plt.show()\n",
    "        print(model.summary())\n",
    "    \n",
    "        model.save('./model.h5')\n",
    "        \n",
    "    # Predict all data\n",
    "    predict = model.predict(x_train)\n",
    "    #results = model.evaluate(x_test, y_test)\n",
    "    #print(results)\n",
    "    # Plot the initial dataset\n",
    "    plot(\"[%s] Predicted data based on x_train\" % (modelname), RefitArousalY(predict))\n",
    "        \n",
    "    #for file in files:\n",
    "    #    compute(modelname, model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}